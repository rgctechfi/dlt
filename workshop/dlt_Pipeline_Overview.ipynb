{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPVVve29bu6Z"
      },
      "source": [
        "# Building a Data Pipeline with dlt\n",
        "\n",
        "In this notebook, we will build a complete data pipeline from scratch using **dlt**.\n",
        "\n",
        "Our goal is simple:\n",
        "\n",
        "‚Üí Fetch real data from an API  \n",
        "‚Üí Turn it into clean relational tables  \n",
        "‚Üí Load it into a database  \n",
        "‚Üí Explore and analyze it  \n",
        "\n",
        "We will use the **Open Library API** as our data source and **DuckDB** as our database.\n",
        "\n",
        "Along the way, you will learn:\n",
        "\n",
        "- What a dlt source is  \n",
        "- What a dlt pipeline does  \n",
        "- How data moves through Extract ‚Üí Normalize ‚Üí Load  \n",
        "- How to inspect and explore the final dataset  \n",
        "\n",
        "By the end, you will understand not just how to run a pipeline, but what happens at each stage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9eCv60qV5PS"
      },
      "source": [
        "## üì¶ Step 0: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arp4d7KZNRTS",
        "outputId": "a9e67a96-7cfb-4901-e762-99208ad436ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m This environment is externally managed\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m To install Python packages system-wide, try apt install\n",
            "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
            "\u001b[31m   \u001b[0m install.\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
            "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
            "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
            "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
            "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
            "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
            "\u001b[31m   \u001b[0m \n",
            "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
            "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
          ]
        }
      ],
      "source": [
        "# install dependencies first (only if uv is not used)\n",
        "!pip -q install dlt[duckdb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7VGYS5hWNKQ"
      },
      "source": [
        "<p>In this notebook we will use:</p>\n",
        "\n",
        "<ul>\n",
        "  <li><strong>dlt</strong> to extract, normalize, and load data</li>\n",
        "  <li><strong>DuckDB</strong> as the destination database (runs locally inside Colab)</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "  DuckDB is great for beginners because it requires no setup and no credentials.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQTSvnvnHWBd"
      },
      "source": [
        "## üìö Step 1: Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFQGLTECWkpn"
      },
      "source": [
        "\n",
        "<p>In this cell we import the libraries we will use throughout the notebook:</p>\n",
        "\n",
        "<ul>\n",
        "  <li><strong>dlt</strong> is the main library for building and running the pipeline</li>\n",
        "  <li><strong>rest_api_source</strong> helps us define an API source using a simple configuration</li>\n",
        "  <li><strong>islice</strong> (from <code>itertools</code>) is a small Python helper for previewing only a few records</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Lm8AbbHBImjI"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import islice\n",
        "from dlt.sources.rest_api import rest_api_source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFoBTwDVhzRL"
      },
      "source": [
        "## üîó Step 2: Define the API Source (Open Library)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdKrEM-VXEY2"
      },
      "source": [
        "<p>\n",
        "  In <strong>dlt</strong>, a <strong>source</strong> is the part of your pipeline that knows how to fetch data from somewhere.\n",
        "  In this notebook, our source fetches data from the <strong>Open Library Search API</strong>.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "  We define the source using <code>rest_api_source</code>, which lets us describe an API in a simple\n",
        "  Python dictionary instead of writing lots of request code.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "  üìñ <strong>Open Library Search API docs:</strong><br>\n",
        "  <a href=\"https://openlibrary.org/dev/docs/api/search\" target=\"_blank\">\n",
        "    https://openlibrary.org/dev/docs/api/search\n",
        "  </a>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hOxkEKy4Kaj4"
      },
      "outputs": [],
      "source": [
        "def openlibrary_source(query: str = \"harry potter\"):\n",
        "\n",
        "    return rest_api_source({\n",
        "        \"client\": {\n",
        "            \"base_url\": \"https://openlibrary.org\",\n",
        "        },\n",
        "        \"resource_defaults\": {\n",
        "            \"primary_key\": \"key\",\n",
        "            \"write_disposition\": \"replace\",\n",
        "        },\n",
        "        \"resources\": [\n",
        "            {\n",
        "                \"name\": \"books\",\n",
        "                \"endpoint\": {\n",
        "                    \"path\": \"search.json\",\n",
        "                    \"params\": {\n",
        "                        \"q\": query,\n",
        "                        \"limit\": 100,\n",
        "                    },\n",
        "                    \"data_selector\": \"docs\",\n",
        "                    \"paginator\": {\n",
        "                        \"type\": \"offset\",\n",
        "                        \"limit\": 100,\n",
        "                        \"offset_param\": \"offset\",\n",
        "                        \"limit_param\": \"limit\",\n",
        "                        \"total_path\": \"numFound\",\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "        ],\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntKAVaEGYFgw"
      },
      "source": [
        "## üîß Step 3: Create the dlt Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bxpFEetGh3lS"
      },
      "outputs": [],
      "source": [
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"ol_demo\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"ol_data\",\n",
        "    progress=\"log\" # logs the pipeline run (Optiona)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7CJ9A2HXsFb"
      },
      "source": [
        "## üîç Understanding the Pipeline\n",
        "\n",
        "At this point we have defined two key building blocks:\n",
        "\n",
        "- **The source** describes where the data comes from and how to fetch it from the API.  \n",
        "- **The pipeline** describes where the data should go (DuckDB) and keeps track of tables, schemas, and run history.  \n",
        "\n",
        "---\n",
        "\n",
        "Instead of running everything at once, we will now run the pipeline in three separate phases so you can clearly see what happens at each stage:\n",
        "\n",
        "1. **Extract**: download raw data from the API  \n",
        "2. **Normalize**: turn nested JSON into relational tables  \n",
        "3. **Load**: write those tables into DuckDB  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OebO693T-U3L"
      },
      "source": [
        "![ETL Diagram](https://github.com/anair123/data-engineering-zoomcamp/blob/workshop/dlt_2026/cohorts/2026/workshops/dlt/images/etl_diagram.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAYgUUJIw-c4"
      },
      "source": [
        "Once these steps make sense, we will run the full workflow again using one command:\n",
        "\n",
        "```python\n",
        "pipeline.run(source)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsfcBA7McJMo"
      },
      "source": [
        "## ‚¨áÔ∏è Step 4: Extract\n",
        "\n",
        "Now we run the first stage of the pipeline: **Extract**.\n",
        "\n",
        "Extract means:\n",
        "\n",
        "- dlt sends requests to the Open Library API\n",
        "- the raw JSON responses are downloaded\n",
        "- the results are stored in dlt‚Äôs local working folder\n",
        "\n",
        "At this stage, the data is **not** in DuckDB yet. We are just confirming that we successfully pulled data from the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yifCIPxSKJZ4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
            "Memory usage: 114.05 MB (53.00%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 0.91s | Rate: 0.00/s\n",
            "books: 100  | Time: 0.00s | Rate: 18236104.35/s\n",
            "Memory usage: 117.30 MB (53.00%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 2.06s | Rate: 0.00/s\n",
            "books: 400  | Time: 1.14s | Rate: 350.12/s\n",
            "Memory usage: 118.18 MB (53.00%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 3.61s | Rate: 0.00/s\n",
            "books: 700  | Time: 2.69s | Rate: 259.86/s\n",
            "Memory usage: 118.30 MB (53.00%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 5.00s | Rate: 0.00/s\n",
            "books: 1000  | Time: 4.09s | Rate: 244.43/s\n",
            "Memory usage: 119.18 MB (53.00%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 6.29s | Rate: 0.00/s\n",
            "books: 1300  | Time: 5.38s | Rate: 241.78/s\n",
            "Memory usage: 120.05 MB (53.00%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 8.73s | Rate: 0.00/s\n",
            "books: 1500  | Time: 7.82s | Rate: 191.87/s\n",
            "Memory usage: 120.05 MB (53.00%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 10.08s | Rate: 0.00/s\n",
            "books: 1800  | Time: 9.16s | Rate: 196.45/s\n",
            "Memory usage: 120.43 MB (53.00%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 11.24s | Rate: 0.00/s\n",
            "books: 2100  | Time: 10.33s | Rate: 203.30/s\n",
            "Memory usage: 121.18 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 12.56s | Rate: 0.00/s\n",
            "books: 2500  | Time: 11.65s | Rate: 214.66/s\n",
            "Memory usage: 122.18 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 13.57s | Rate: 0.00/s\n",
            "books: 2800  | Time: 12.65s | Rate: 221.29/s\n",
            "Memory usage: 122.18 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 14.61s | Rate: 0.00/s\n",
            "books: 3100  | Time: 13.69s | Rate: 226.38/s\n",
            "Memory usage: 122.55 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 15.87s | Rate: 0.00/s\n",
            "books: 3400  | Time: 14.96s | Rate: 227.27/s\n",
            "Memory usage: 123.30 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 17.38s | Rate: 0.00/s\n",
            "books: 3700  | Time: 16.46s | Rate: 224.74/s\n",
            "Memory usage: 123.80 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 1/1 (100.0%) | Time: 17.79s | Rate: 0.06/s\n",
            "books: 3759  | Time: 16.88s | Rate: 222.73/s\n",
            "Memory usage: 123.87 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
            "Memory usage: 123.87 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
            "_dlt_pipeline_state: 1  | Time: 0.00s | Rate: 524288.00/s\n",
            "Memory usage: 123.87 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 1/1 (100.0%) | Time: 0.01s | Rate: 74.07/s\n",
            "_dlt_pipeline_state: 1  | Time: 0.01s | Rate: 86.23/s\n",
            "Memory usage: 123.87 MB (53.10%) | CPU usage: 0.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "extract_info = pipeline.extract(openlibrary_source())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLRRVLnLcNgl"
      },
      "source": [
        "---\n",
        "\n",
        "### What we will print\n",
        "\n",
        "After extraction, we will print a small summary showing:\n",
        "\n",
        "- which **resources** were extracted\n",
        "- which **tables** will be created later\n",
        "- how many rows were extracted per resource\n",
        "\n",
        "This helps confirm that the pipeline is working before we move on to normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtDasHRNNNN0",
        "outputId": "51c71eeb-5435-40a1-8728-ea48c59bfd58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resources: ['books']\n",
            "Tables: ['books']\n",
            "Load ID: 1772033868.1635435\n",
            "\n",
            "Resource: books\n",
            "rows extracted: 3759\n",
            "\n"
          ]
        }
      ],
      "source": [
        "load_id = extract_info.loads_ids[-1]\n",
        "m = extract_info.metrics[load_id][0]\n",
        "\n",
        "print(\"Resources:\", list(m[\"resource_metrics\"].keys()))\n",
        "print(\"Tables:\", list(m[\"table_metrics\"].keys()))\n",
        "print(\"Load ID:\", load_id)\n",
        "print()\n",
        "\n",
        "for resource, rm in m[\"resource_metrics\"].items():\n",
        "    print(f\"Resource: {resource}\")\n",
        "    print(f\"rows extracted: {rm.items_count}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6MwYtznc3UX"
      },
      "source": [
        "### What you should see after Extract\n",
        "\n",
        "In our case, Extract shows only **one resource and one table**:\n",
        "\n",
        "- **Resources:** `['books']`  \n",
        "- **Tables:** `['books']`\n",
        "\n",
        "That is expected.\n",
        "\n",
        "The `search` endpoint returns a list of book results, so dlt stores those rows in a single table called `books`. The interesting part comes next, because many fields inside each row are lists or nested objects. Those will turn into additional tables during **Normalize**.\n",
        "\n",
        "Example output:\n",
        "\n",
        "- **25 rows extracted** means we pulled 25 search results (books)  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQVLZMcyXWkm"
      },
      "source": [
        "## üîÑ Step 5: Normalize\n",
        "\n",
        "Now we run **Normalize**. This is where dlt transforms raw JSON into a clean relational structure.\n",
        "\n",
        "During normalization, dlt does three key things:\n",
        "\n",
        "### 1. Adds Tracking Columns to the Main Table\n",
        "\n",
        "dlt adds special columns to every table:\n",
        "- `_dlt_id`: A unique identifier for each row\n",
        "- `_dlt_load_id`: Links each row to the load job that created it\n",
        "\n",
        "### 2. Flattens Nested Data into Child Tables\n",
        "\n",
        "APIs often return nested JSON. For example, a book can have multiple authors (a list), multiple editions, and multiple identifiers.\n",
        "\n",
        "dlt flattens these nested structures into separate **child tables** with names like:\n",
        "- `books__author_name`\n",
        "- `books__author_key`\n",
        "- `books__language`\n",
        "\n",
        "Each child table has a `_dlt_parent_id` column that references `_dlt_id` in the parent table. This is how dlt maintains relationships.\n",
        "\n",
        "### 3. Creates Metadata Tables\n",
        "\n",
        "dlt also creates internal tables to track pipeline state:\n",
        "- `_dlt_loads`: Tracks load history (when data was loaded, status)\n",
        "- `_dlt_pipeline_state`: Stores pipeline state for incremental loading\n",
        "- `_dlt_version`: Tracks schema versions\n",
        "\n",
        "In the next cell, we will print a summary showing which tables were created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LCmiiG3tXXwh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------- Normalize rest_api in 1772033868.1635435 -------------------\n",
            "Files: 0/2 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
            "Memory usage: 124.87 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------- Normalize rest_api in 1772033868.1635435 -------------------\n",
            "Files: 0/2 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
            "Items: 0  | Time: 0.00s | Rate: 0.00/s\n",
            "Memory usage: 124.87 MB (53.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------- Normalize rest_api in 1772033868.1635435 -------------------\n",
            "Files: 10/2 (500.0%) | Time: 0.38s | Rate: 26.50/s\n",
            "Items: 23075  | Time: 0.38s | Rate: 61240.27/s\n",
            "Memory usage: 130.05 MB (53.20%) | CPU usage: 0.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "normalize_info = pipeline.normalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kNiY112Xvuk",
        "outputId": "502bff6b-edb2-4bd8-a9e9-1f1b88f20c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load ID: 1772033868.1635435\n",
            "\n",
            "Tables created/updated:\n",
            "  - books: 3759 rows\n",
            "  - books__author_key: 4637 rows\n",
            "  - books__author_name: 4637 rows\n",
            "  - books__ia: 3431 rows\n",
            "  - books__ia_collection: 2730 rows\n",
            "  - books__language: 3748 rows\n",
            "  - books__id_standard_ebooks: 12 rows\n",
            "  - books__id_librivox: 64 rows\n",
            "  - books__id_project_gutenberg: 56 rows\n"
          ]
        }
      ],
      "source": [
        "load_id = normalize_info.loads_ids[-1]\n",
        "m = normalize_info.metrics[load_id][0]\n",
        "\n",
        "print(\"Load ID:\", load_id)\n",
        "print()\n",
        "\n",
        "print(\"Tables created/updated:\")\n",
        "for table_name, tm in m[\"table_metrics\"].items():\n",
        "    # skip dlt internal tables to keep it beginner-friendly\n",
        "    if table_name.startswith(\"_dlt\"):\n",
        "        continue\n",
        "    print(f\"  - {table_name}: {tm.items_count} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctHuJ0yEdNaq"
      },
      "source": [
        "### What happened during Normalize?\n",
        "\n",
        "After running `pipeline.normalize()`, we now see multiple tables instead of just one.\n",
        "\n",
        "Tables created/updated:\n",
        "\n",
        "- `books`\n",
        "- `books__author_key`\n",
        "- `books__author_name`\n",
        "- `books__editions__docs`\n",
        "- `books__editions__docs__language`\n",
        "- `books__ia`\n",
        "\n",
        "---\n",
        "\n",
        "### What does this mean?\n",
        "\n",
        "We started with **N book search results** in the `books` table.\n",
        "\n",
        "During normalization:\n",
        "\n",
        "- Each book may have **more than N authors**, so those were split into:\n",
        "  - `books__author_name`\n",
        "  - `books__author_key`\n",
        "\n",
        "- Each book may contain **edition information**, which became:\n",
        "  - `books__editions__docs`\n",
        "\n",
        "- Some editions contain **language information**, which became:\n",
        "  - `books__editions__docs__language`\n",
        "\n",
        "- The `ia` field (Internet Archive IDs) is a list, so it became:\n",
        "  - `books__ia`\n",
        "\n",
        "This is the key moment in the pipeline.\n",
        "\n",
        "The data has been transformed from nested JSON into a **relational structure** with multiple linked tables. This makes it much easier to query and analyze.\n",
        "\n",
        "---\n",
        "\n",
        "### Schema Visualization\n",
        "\n",
        "dlt can render the schema as a visual diagram. Run the next cell to see the parent-child table relationships:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2YK1a7qP-U3M",
        "outputId": "6ce3a181-3105-46da-86eb-ee95e31260dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "default_schema_name: rest_api\n",
            "schemas: ['rest_api']\n",
            "<dlt.Schema(name='rest_api', version=2, tables=['_dlt_version', '_dlt_loads', 'books', '_dlt_pipeline_state', 'books__author_key', 'books__author_name', 'books__ia', 'books__ia_collection', 'books__language', 'books__id_standard_ebooks', 'books__id_librivox', 'books__id_project_gutenberg'], version_hash='ZJIabaQJ9DAYgsR04wEVeXOgU80roBUfdvrR2YoBEyU=')>\n"
          ]
        }
      ],
      "source": [
        "# Display schema\n",
        "pipeline.default_schema\n",
        "print(\"default_schema_name:\", pipeline.default_schema_name)\n",
        "print(\"schemas:\", list(pipeline.schemas.keys()))\n",
        "print(pipeline.default_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ5QzSnYdidK"
      },
      "source": [
        "## üì§ Step 6: Load\n",
        "\n",
        "Now we run the final stage of the pipeline: **Load**.\n",
        "\n",
        "Load means:\n",
        "\n",
        "- dlt creates tables in DuckDB (if they do not already exist)\n",
        "- the normalized rows are inserted into those tables\n",
        "- the pipeline records the load in its internal tracking tables\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "d9Xb67c5XfL5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------- Load rest_api in 1772033868.1635435 ----------------------\n",
            "Jobs: 0/10 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
            "Memory usage: 153.74 MB (54.30%) | CPU usage: 0.00%\n",
            "\n",
            "--------------------- Load rest_api in 1772033868.1635435 ----------------------\n",
            "Jobs: 10/10 (100.0%) | Time: 0.96s | Rate: 10.38/s\n",
            "Memory usage: 178.20 MB (55.20%) | CPU usage: 0.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "load_info = pipeline.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehkz8lESGGdm"
      },
      "source": [
        "\n",
        "After this step, the data is fully stored in the database and ready to query.\n",
        "\n",
        "At this point:\n",
        "\n",
        "- The `books` table contains our books\n",
        "- The related tables (such as `books__author_name` and `books__editions__docs`) contain the exploded nested data\n",
        "- Everything is now queryable using `pipeline.dataset()` or SQL\n",
        "\n",
        "This is the moment where the data officially moves from ‚Äúpipeline processing‚Äù into a database you can explore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBznxM00eCOF"
      },
      "source": [
        "## üöÄ Step 7: Run the Full Pipeline\n",
        "\n",
        "Now that we have walked through each step individually, we can run the entire workflow using a single command:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YQLigkh-f7Ey"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
            "Memory usage: 169.66 MB (55.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 0.94s | Rate: 0.00/s\n",
            "books: 100  | Time: 0.00s | Rate: 32263876.92/s\n",
            "Memory usage: 169.79 MB (55.10%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 2.66s | Rate: 0.00/s\n",
            "books: 400  | Time: 1.72s | Rate: 232.37/s\n",
            "Memory usage: 170.41 MB (55.30%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 4.34s | Rate: 0.00/s\n",
            "books: 600  | Time: 3.41s | Rate: 176.14/s\n",
            "Memory usage: 171.29 MB (55.50%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 5.54s | Rate: 0.00/s\n",
            "books: 900  | Time: 4.60s | Rate: 195.59/s\n",
            "Memory usage: 171.54 MB (55.50%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 6.86s | Rate: 0.00/s\n",
            "books: 1200  | Time: 5.92s | Rate: 202.59/s\n",
            "Memory usage: 171.79 MB (55.50%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 8.35s | Rate: 0.00/s\n",
            "books: 1400  | Time: 7.41s | Rate: 188.90/s\n",
            "Memory usage: 172.29 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 9.59s | Rate: 0.00/s\n",
            "books: 1700  | Time: 8.65s | Rate: 196.57/s\n",
            "Memory usage: 173.16 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 10.65s | Rate: 0.00/s\n",
            "books: 2000  | Time: 9.71s | Rate: 205.91/s\n",
            "Memory usage: 173.79 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 11.72s | Rate: 0.00/s\n",
            "books: 2300  | Time: 10.78s | Rate: 213.37/s\n",
            "Memory usage: 173.79 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 12.79s | Rate: 0.00/s\n",
            "books: 2600  | Time: 11.86s | Rate: 219.30/s\n",
            "Memory usage: 173.91 MB (55.50%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 14.06s | Rate: 0.00/s\n",
            "books: 2900  | Time: 13.12s | Rate: 221.08/s\n",
            "Memory usage: 174.66 MB (55.50%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 15.41s | Rate: 0.00/s\n",
            "books: 3200  | Time: 14.47s | Rate: 221.10/s\n",
            "Memory usage: 175.41 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 18.63s | Rate: 0.00/s\n",
            "books: 3500  | Time: 17.70s | Rate: 197.78/s\n",
            "Memory usage: 175.41 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 0/1 (0.0%) | Time: 19.90s | Rate: 0.00/s\n",
            "books: 3759  | Time: 18.96s | Rate: 198.26/s\n",
            "Memory usage: 175.41 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "------------------------------- Extract rest_api -------------------------------\n",
            "Resources: 1/1 (100.0%) | Time: 19.95s | Rate: 0.05/s\n",
            "books: 3759  | Time: 19.01s | Rate: 197.71/s\n",
            "Memory usage: 177.04 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "------------------- Normalize rest_api in 1772034231.1452715 -------------------\n",
            "Files: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
            "Memory usage: 177.16 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "------------------- Normalize rest_api in 1772034231.1452715 -------------------\n",
            "Files: 0/1 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
            "Items: 0  | Time: 0.00s | Rate: 0.00/s\n",
            "Memory usage: 177.16 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "------------------- Normalize rest_api in 1772034231.1452715 -------------------\n",
            "Files: 9/1 (900.0%) | Time: 0.40s | Rate: 22.26/s\n",
            "Items: 23074  | Time: 0.40s | Rate: 57100.81/s\n",
            "Memory usage: 180.91 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "--------------------- Load rest_api in 1772034231.1452715 ----------------------\n",
            "Jobs: 0/9 (0.0%) | Time: 0.00s | Rate: 0.00/s\n",
            "Memory usage: 180.91 MB (55.60%) | CPU usage: 0.00%\n",
            "\n",
            "--------------------- Load rest_api in 1772034231.1452715 ----------------------\n",
            "Jobs: 9/9 (100.0%) | Time: 0.78s | Rate: 11.54/s\n",
            "Memory usage: 187.30 MB (55.80%) | CPU usage: 0.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "load_info = pipeline.run(openlibrary_source())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbLkA8W7eNPb"
      },
      "source": [
        "<h3>What does <code>pipeline.run()</code> do?</h3>\n",
        "\n",
        "<p>\n",
        "  <code>pipeline.run()</code> simply combines the three steps we already executed manually:\n",
        "</p>\n",
        "\n",
        "<ol>\n",
        "  <li><strong>Extract</strong> ‚Äì fetch data from the Open Library API</li>\n",
        "  <li><strong>Normalize</strong> ‚Äì convert nested JSON into relational tables</li>\n",
        "  <li><strong>Load</strong> ‚Äì write those tables into DuckDB</li>\n",
        "</ol>\n",
        "\n",
        "<p>In other words, this:</p>\n",
        "\n",
        "<pre><code>pipeline.run(source)</code></pre>\n",
        "\n",
        "<p>is equivalent to:</p>\n",
        "\n",
        "<pre><code>pipeline.extract(source)\n",
        "pipeline.normalize()\n",
        "pipeline.load()</code></pre>\n",
        "\n",
        "<p>\n",
        "  There is no hidden magic. It just runs the full ELT process in order.\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ViMq6gIfJj_"
      },
      "source": [
        "## üîé Step 8: Inspect the Loaded Data\n",
        "\n",
        "Now that the data is loaded into DuckDB, we can inspect it using `pipeline.dataset()`.\n",
        "\n",
        "This gives us a convenient Python interface for exploring the tables that dlt created, without writing SQL.\n",
        "\n",
        "---\n",
        "\n",
        "### List available tables\n",
        "\n",
        "First, let‚Äôs see what tables exist in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bmnrK1aVZXPO"
      },
      "outputs": [],
      "source": [
        "ds = pipeline.dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV6J6AtBf0xq",
        "outputId": "19ad26bf-f34a-4f8e-c30c-5acd3342c3c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['books',\n",
              " 'books__author_key',\n",
              " 'books__author_name',\n",
              " 'books__ia',\n",
              " 'books__ia_collection',\n",
              " 'books__language',\n",
              " 'books__id_standard_ebooks',\n",
              " 'books__id_librivox',\n",
              " 'books__id_project_gutenberg',\n",
              " '_dlt_version',\n",
              " '_dlt_loads',\n",
              " '_dlt_pipeline_state']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds.tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "WLa4yN7lf1TF",
        "outputId": "d2da841b-a8bf-461f-a011-eb1db644656f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cover_edition_key</th>\n",
              "      <th>cover_i</th>\n",
              "      <th>ebook_access</th>\n",
              "      <th>edition_count</th>\n",
              "      <th>first_publish_year</th>\n",
              "      <th>has_fulltext</th>\n",
              "      <th>key</th>\n",
              "      <th>lending_edition_s</th>\n",
              "      <th>lending_identifier_s</th>\n",
              "      <th>public_scan_b</th>\n",
              "      <th>title</th>\n",
              "      <th>_dlt_load_id</th>\n",
              "      <th>_dlt_id</th>\n",
              "      <th>subtitle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OL61027601M</td>\n",
              "      <td>15155833</td>\n",
              "      <td>borrowable</td>\n",
              "      <td>397</td>\n",
              "      <td>1997</td>\n",
              "      <td>True</td>\n",
              "      <td>/works/OL82563W</td>\n",
              "      <td>OL38565767M</td>\n",
              "      <td>harrypotterylapi0000rowl_q5r6</td>\n",
              "      <td>False</td>\n",
              "      <td>Harry Potter and the Philosopher's Stone</td>\n",
              "      <td>1772034231.1452715</td>\n",
              "      <td>4NoSitrDvSc68Q</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OL26378158M</td>\n",
              "      <td>15158660</td>\n",
              "      <td>printdisabled</td>\n",
              "      <td>144</td>\n",
              "      <td>2007</td>\n",
              "      <td>True</td>\n",
              "      <td>/works/OL82586W</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>Harry Potter and the Deathly Hallows</td>\n",
              "      <td>1772034231.1452715</td>\n",
              "      <td>8A6CSf4X2TZ08A</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OL26234270M</td>\n",
              "      <td>10580435</td>\n",
              "      <td>borrowable</td>\n",
              "      <td>279</td>\n",
              "      <td>1999</td>\n",
              "      <td>True</td>\n",
              "      <td>/works/OL82536W</td>\n",
              "      <td>OL48101764M</td>\n",
              "      <td>bdrc-W8LS66814</td>\n",
              "      <td>False</td>\n",
              "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
              "      <td>1772034231.1452715</td>\n",
              "      <td>WjCwQXPIRCE9Vg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  cover_edition_key   cover_i   ebook_access  edition_count  \\\n",
              "0       OL61027601M  15155833     borrowable            397   \n",
              "1       OL26378158M  15158660  printdisabled            144   \n",
              "2       OL26234270M  10580435     borrowable            279   \n",
              "\n",
              "   first_publish_year  has_fulltext              key lending_edition_s  \\\n",
              "0                1997          True  /works/OL82563W       OL38565767M   \n",
              "1                2007          True  /works/OL82586W               NaN   \n",
              "2                1999          True  /works/OL82536W       OL48101764M   \n",
              "\n",
              "            lending_identifier_s  public_scan_b  \\\n",
              "0  harrypotterylapi0000rowl_q5r6          False   \n",
              "1                            NaN          False   \n",
              "2                 bdrc-W8LS66814          False   \n",
              "\n",
              "                                      title        _dlt_load_id  \\\n",
              "0  Harry Potter and the Philosopher's Stone  1772034231.1452715   \n",
              "1      Harry Potter and the Deathly Hallows  1772034231.1452715   \n",
              "2  Harry Potter and the Prisoner of Azkaban  1772034231.1452715   \n",
              "\n",
              "          _dlt_id subtitle  \n",
              "0  4NoSitrDvSc68Q      NaN  \n",
              "1  8A6CSf4X2TZ08A      NaN  \n",
              "2  WjCwQXPIRCE9Vg      NaN  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = ds.books.df()      # main table\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWFqaH2wgCWR"
      },
      "source": [
        "## üí° Conclusion\n",
        "\n",
        "### What dlt handled for us\n",
        "\n",
        "‚úî API requests  \n",
        "‚úî JSON normalization  \n",
        "‚úî Table creation  \n",
        "‚úî Database loading  \n",
        "‚úî Simple dataset inspection  \n",
        "\n",
        "---\n",
        "\n",
        "### But there are still friction points\n",
        "\n",
        "‚Ä¢ Getting the REST API config exactly right  \n",
        "‚Ä¢ Remembering paginator syntax  \n",
        "‚Ä¢ Remembering how to inspect tables  \n",
        "‚Ä¢ Debugging schema or pagination issues  \n",
        "‚Ä¢ Writing Python or SQL to get insights  \n",
        "\n",
        "It works... but it still takes effort.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Up: LLM-Powered Workflows\n",
        "\n",
        "dlt now integrates LLMs directly into the workflow to make:\n",
        "\n",
        "‚Ä¢ Pipeline runs easier  \n",
        "‚Ä¢ Debugging faster  \n",
        "‚Ä¢ Schema inspection simpler  \n",
        "‚Ä¢ Data analysis more natural  \n",
        "\n",
        "Instead of writing glue code, you can use natural language.\n",
        "\n",
        "In the workshop, we will see what that looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BweSVO3igErN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "dlt-workshop (3.13.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
