<h1 align="center">
  <span>ùôôùôñùô©ùôñ ùô°ùô§ùôñùôô ùô©ùô§ùô§ùô°</span>
  <br />
  <img src="./ressources/pictures/dlt_logo.png" alt="dlt logo" width="42" />
</h1>

<p align="center">
  <img src="./ressources/pictures/dlt_banner.png" alt="dlt banner" width="720" />
</p>

<p align="center">
  <em>From APIs to Warehouses: AI-Assisted Data Ingestion with dlt</em>
</p>

## About dlt

[dlt](https://dlthub.com/docs) (data load tool) is an open-source Python library for building reliable ELT pipelines. It helps you extract data from APIs or databases, normalize it into clean relational tables, and load it into destinations like DuckDB.

In this quickstart, you'll use an AI-powered IDE to build a complete pipeline from the Open Library API to a local DuckDB warehouse.

**What You'll Build**

By the end of this quickstart, you will have:

1. A working dlt pipeline that extracts data from the [Open Library API](https://openlibrary.org/developers/api)
2. Normalized relational tables stored in DuckDB
3. The ability to query, inspect, and visualize your data
4. Experience using AI-assisted development for data engineering

**No API key required!** The Open Library API is completely open and doesn't require authentication. You can start building immediately.

**Key Concepts**

- **Source**: An API or database you extract data from (e.g., Open Library API)
- **Pipeline**: A dlt object that orchestrates extraction, normalization, and loading
- **Extract**: Fetches data from the source using REST API, connectors, or custom code
- **Normalize**: Transforms raw data into relational tables
- **Load**: Stores normalized data in DuckDB (or any supported destination)

## Quickstart: Open Library Pipeline (DuckDB)

This section explains how to build and run a dlt pipeline that extracts data from the Open Library API and loads it into DuckDB.

### 1) Prerequisites

- **Python 3.11+** installed
- **uv** package manager (recommended) or **pip**
- An agentic IDE: [Cursor](https://cursor.sh) (recommended), [Windsurf](https://codeium.com/windsurf), or VS Code + GitHub Copilot
- Basic understanding of [dlt concepts](https://dlthub.com/docs)

```bash
# Verify Python version
python --version  # Should be 3.11+

# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2) Project Setup

Create a new project folder and initialize the dlt workspace:

```bash
mkdir my-dlt-pipeline
cd my-dlt-pipeline

# Install dlt workspace tools
pip install "dlt[workspace]"

# Initialize the dlt project with Open Library source
dlt init dlthub:open_library duckdb
```

This creates the following structure:

- `open_library_pipeline.py`: Main pipeline code (generated by `dlt init`)
- `.dlt/secrets.toml`: Credentials configuration (empty for Open Library)
- `.dlt/config.toml`: Environment settings
- `duckdb.duckdb`: Local DuckDB database file

### 3) Essential Commands

**Run the pipeline:**

```bash
# Generate and run the pipeline with AI assistance
# Use your agentic IDE (Cursor, Windsurf, etc.) to prompt:
# "Generate a REST API Source for Open Library API with books endpoint"

python open_library_pipeline.py
```

**Inspect loaded data:**

```bash
# Launch the dlt dashboard
dlt pipeline open_library_pipeline show
```

The dashboard shows:
- Pipeline state and run history
- Schemas, tables, and columns
- Data preview and lineage
- Load statistics and errors

### 4) Query Your Data

Query the loaded data directly from Python:

```bash
# Example: Count loaded books
dlt pipeline open_library_pipeline query "SELECT COUNT(*) as total_books FROM books;"
```

### 5) Next Steps (Bonus)

**Build Visualizations:**

Create interactive reports with [marimo](https://marimo.io/) and [ibis](https://ibis-project.org/):

```bash
# Prompt your AI agent:
# "Create a marimo notebook that visualizes the top 10 authors by book count.
#  Use ibis for data access."
```

**Explore the dlt Workspace:**

```bash
# View available sources
dlt workspace list

# Read the Open Library source documentation
dlt workspace info dlthub:open_library
```

### 6) Important Notes

- No API key required for Open Library API
- Use your agentic IDE to generate code‚Äîthe AI understands dlt syntax
- The dlt MCP server provides access to documentation and pipeline metadata
- DuckDB is completely local‚Äîno cloud costs
- To reset and start fresh:

```bash
rm duckdb.duckdb
python open_library_pipeline.py
```

---

## Resources

| Resource | Link |
|----------|------|
| dlt Documentation | [dlthub.com/docs](https://dlthub.com/docs) |
| Open Library Workspace Guide | [dlthub.com/workspace/source/open-library](https://dlthub.com/workspace/source/open-library) |
| dlt Dashboard Docs | [dlthub.com/docs/general-usage/dashboard](https://dlthub.com/docs/general-usage/dashboard) |
| marimo + dlt Guide | [dlthub.com/docs/general-usage/dataset-access/marimo](https://dlthub.com/docs/general-usage/dataset-access/marimo) |
| Open Library API | [openlibrary.org/developers/api](https://openlibrary.org/developers/api) |

---

*Created with [dltHub](https://dlthub.com)*
